{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friendly036/test/blob/main/mini_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a254cc9",
      "metadata": {
        "id": "2a254cc9"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Komoran\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.translate.bleu_score import sentence_bleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0639fee",
      "metadata": {
        "id": "d0639fee"
      },
      "outputs": [],
      "source": [
        "komoran = Komoran()\n",
        "\n",
        "# 패턴 파일 읽기\n",
        "db = pd.read_table('db_KOMORAN.txt', sep = \"\\t\", engine='python', encoding = \"utf-8\")\n",
        "db = pd.Series(db.main_action.values, index=db.corpus_pattern).to_dict()\n",
        "\n",
        "action_ne_f = open(\"Action_NE_Table.txt\", 'r', encoding=\"UTF-8\")\n",
        "lines = action_ne_f.readlines()\n",
        "action_ne_f.close()\n",
        "action_table_NE = []\n",
        "\n",
        "# 액션테이블 읽기\n",
        "for line in lines:\n",
        "    items = line.strip().split(':\\t')\n",
        "    if len(items) == 2:\n",
        "        action_table_NE.append([items[0], items[1]]) # [action , entity]\n",
        "    else:\n",
        "        action_table_NE.append([items[0][:-1], ''])\n",
        "\n",
        "# 엔티티 파일 읽기\n",
        "entity_dict = {}    # {\"home_device\":[{\"device_status\":[에어컨, 에어컨디셔너, ...]}]}\n",
        "for action, entity_files in action_table_NE:\n",
        "    if entity_files:\n",
        "        for entity_file in entity_files.split(\", \"):\n",
        "            if not entity_file in entity_dict:\n",
        "                f = open(entity_file+\".txt\", 'r', encoding=\"UTF-8\")\n",
        "                ett_lines = f.readlines()\n",
        "                ett_lines = list(map(lambda s: s.strip(), ett_lines))\n",
        "                ett_name_dic = {}  # {\"device_status\":[에어컨:aircon, 에어컨디셔너:aircon, 벽걸이에어컨:aircon...]}\n",
        "                ett_name = ''\n",
        "                etts = []\n",
        "                for line in ett_lines:\n",
        "                    if line[0:4] == \"<ne>\":\n",
        "                        if etts:\n",
        "                            ett_name_dic[ett_name] = etts\n",
        "                            etts = []\n",
        "                        ett_name = line.split('\\t')[1]\n",
        "                    else:\n",
        "                        if line and not line.startswith('#'):\n",
        "                            etts.append(line.strip())\n",
        "                \n",
        "                if etts:\n",
        "                    ett_name_dic[ett_name] = etts        \n",
        "                entity_dict[entity_file] = ett_name_dic\n",
        "                f.close()\n",
        "\n",
        "    else:\n",
        "        entity_dict[entity_files] = {}\n",
        "\n",
        "# print(entity_dict.keys())\n",
        "\n",
        "def detect_action(utterance):\n",
        "    output_dict = {\"action\":'None', \"pos\":'', \"matched pattern\":'None', \"detected_NE\":defaultdict(list), \"all_utterances\":[]}\n",
        "    pos_list = komoran.pos(utterance)\n",
        "    output_dict[\"pos\"] = pos_list\n",
        "    parsed_utters = ''  # 각 메인액션별 발화 분석 결과\n",
        "    parsed_patterns = []\n",
        "\n",
        "    for action, entity_files in action_table_NE: # 엔티티 치환은 이 루프(=액션 수)만큼\n",
        "        parsed_words = []\n",
        "        for pos_tup in pos_list:    # 각 단어별로\n",
        "            is_replaced = False\n",
        "            for entity_file in entity_files.split(\", \"): # 엔티티 일치 검사\n",
        "                if pos_tup[1] in [\"NNG\", \"NNP\", \"NR\", \"SL\", \"SN\"]:            \n",
        "                    for entity_name in entity_dict[entity_file].keys(): # [<ne> home_device, device_status, device_mode, ...]\n",
        "                        for entity in entity_dict[entity_file][entity_name]:    # [에어컨, 에어컨디셔너, ...]\n",
        "                            if pos_tup[0] == entity.split(\":\")[0]:\n",
        "                                parsed_words.append(entity_name)\n",
        "                                is_replaced = True\n",
        "                                output_dict[\"detected_NE\"][action].append(entity)\n",
        "                                break\n",
        "                        if is_replaced:\n",
        "                            break\n",
        "            if not is_replaced and pos_tup[1] in [\"NNG\", \"NNP\", \"NR\", \"VV\", \"VA\", \"MM\", \"MAG\", \"SL\", \"XR\", \"SN\"]:   # 명/동/ 형용사일 때만 인식\n",
        "                parsed_words.append(pos_tup[0])\n",
        "            elif pos_tup == ('말', 'VX'):\n",
        "                parsed_words.append(pos_tup[0])\n",
        "            elif pos_tup == ('꺼', 'NNB'):\n",
        "                parsed_words.append(pos_tup[0])                \n",
        "\n",
        "        parsed_utters += action+\": [\"+', '.join(parsed_words)+\"]\\r\\n\"\n",
        "        # parsed_utters.append(action+\": [\"+', '.join(parsed_words)+\"]\")\n",
        "        parsed_patterns.append(' '.join(parsed_words))\n",
        "    \n",
        "    output_dict[\"all_utterances\"] = parsed_utters\n",
        "\n",
        "    # 패턴 매칭\n",
        "    for parsed_pattern in parsed_patterns:\n",
        "        for pattern, action in db.items():\n",
        "            if pattern == parsed_pattern:\n",
        "                output_dict[\"action\"] = action\n",
        "                output_dict[\"matched pattern\"] = pattern\n",
        "                return output_dict\n",
        "\n",
        "    # 여기 들어오면 액션은 None\n",
        "        \n",
        "    ut = parsed_pattern\n",
        "    reference_sequences = [[\"tv_search_title\", [\"나의 해방일지\", \"꼬리에 꼬리를 무는 이야기\", \"찾 찾아보 틀 켜\"]], [\"tv_play\",[\"번 채널 번 틀\"]], [\"tv_volume_up\", [\"볼륨 소리 음량 키우 늘리 크 올리 높이\"]], [\"tv_volume_down\", [\"볼륨 소리 음량 줄이 낮 낮추 작 적\"]], [\"sh_device_power_on\", [\"켜 틀 돌리 작동\"]], [\"sh_device_power_off\", [\"끄 꺼 종료 정지\"]], [\"wt_search_weather\", [\"날씨 기상예보 일기예보\"]]]    # 콜랩에 올린 결과\n",
        "\n",
        "    bleu_dict = {\"tv_search_title\":0, \"tv_play\":0, \"tv_volume_up\":0, \"tv_volume_down\":0, \"sh_device_power_on\":0, \"sh_device_power_off\":0, \"wt_search_weather\":0}\n",
        "    for reference_sequence in reference_sequences:\n",
        "        bleu_dict[reference_sequence[0]] = sentence_bleu(reference_sequence[1], ut, weights=(3/6, 2/6, 1/6))\n",
        "        if output_dict[\"detected_NE\"][reference_sequence[0]]:\n",
        "            bleu_dict[reference_sequence[0]] *= 3*len(output_dict[\"detected_NE\"][reference_sequence[0]])\n",
        "\n",
        "    bleu_dict = sorted(bleu_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "    output_dict[\"action\"] = bleu_dict[0][0]\n",
        "\n",
        "    return output_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ebf8d50",
      "metadata": {
        "id": "9ebf8d50",
        "outputId": "21ab0fa0-f28a-4378-a201-cbafd97665ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'action': 'wt_search_weather',\n",
              " 'pos': [('부산', 'NNP'), ('날씨', 'NNP')],\n",
              " 'matched pattern': 'location 날씨',\n",
              " 'detected_NE': defaultdict(list,\n",
              "             {'sh_device_power_on': ['부산'],\n",
              "              'sh_device_power_off': ['부산'],\n",
              "              'wt_search_weather': ['부산']}),\n",
              " 'all_utterances': 'tv_volume_up: [부산, 날씨]\\r\\ntv_volume_down: [부산, 날씨]\\r\\nsh_device_power_on: [location, 날씨]\\r\\nsh_device_power_off: [location, 날씨]\\r\\nwt_search_weather: [location, 날씨]\\r\\n'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detect_action(\"부산 날씨\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bcb626e",
      "metadata": {
        "id": "9bcb626e",
        "outputId": "7102e677-48ae-4a2e-e6ce-9a615d4624d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "영어 stop word 개수 : 179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kisa0\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "print(\"영어 stop word 개수 :\", len(nltk.corpus.stopwords.words(\"english\")))\n",
        "nltk.corpus.stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00e6b47",
      "metadata": {
        "id": "d00e6b47",
        "outputId": "8d0c05b8-b751-448e-d5ae-8ea2de4728c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'EC': '연결 어미',\n",
              " 'EF': '종결 어미',\n",
              " 'EP': '선어말어미',\n",
              " 'ETM': '관형형 전성 어미',\n",
              " 'ETN': '명사형 전성 어미',\n",
              " 'IC': '감탄사',\n",
              " 'JC': '접속 조사',\n",
              " 'JKB': '부사격 조사',\n",
              " 'JKC': '보격 조사',\n",
              " 'JKG': '관형격 조사',\n",
              " 'JKO': '목적격 조사',\n",
              " 'JKQ': '인용격 조사',\n",
              " 'JKS': '주격 조사',\n",
              " 'JKV': '호격 조사',\n",
              " 'JX': '보조사',\n",
              " 'MAG': '일반 부사',\n",
              " 'MAJ': '접속 부사',\n",
              " 'MM': '관형사',\n",
              " 'NA': '분석불능범주',\n",
              " 'NF': '명사추정범주',\n",
              " 'NNB': '의존 명사',\n",
              " 'NNG': '일반 명사',\n",
              " 'NNP': '고유 명사',\n",
              " 'NP': '대명사',\n",
              " 'NR': '수사',\n",
              " 'NV': '용언추정범주',\n",
              " 'SE': '줄임표',\n",
              " 'SF': '마침표, 물음표, 느낌표',\n",
              " 'SH': '한자',\n",
              " 'SL': '외국어',\n",
              " 'SN': '숫자',\n",
              " 'SO': '붙임표(물결,숨김,빠짐)',\n",
              " 'SP': '쉼표,가운뎃점,콜론,빗금',\n",
              " 'SS': '따옴표,괄호표,줄표',\n",
              " 'SW': '기타기호 (논리수학기호,화폐기호)',\n",
              " 'VA': '형용사',\n",
              " 'VCN': '부정 지정사',\n",
              " 'VCP': '긍정 지정사',\n",
              " 'VV': '동사',\n",
              " 'VX': '보조 용언',\n",
              " 'XPN': '체언 접두사',\n",
              " 'XR': '어근',\n",
              " 'XSA': '형용사 파생 접미사',\n",
              " 'XSN': '명사파생 접미사',\n",
              " 'XSV': '동사 파생 접미사'}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "komoran.tagset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ee50038",
      "metadata": {
        "id": "7ee50038"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}